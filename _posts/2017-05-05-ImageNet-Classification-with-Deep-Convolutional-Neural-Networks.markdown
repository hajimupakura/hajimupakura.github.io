AlexNet has a very similar architecture to LeNet, with the added insight of stacking multiple convolution layers before the pooling and activation layers, rather than alternating layers of convolution, activation, then pooling. It is also deeper and bigger. It also had dropout during training which reduced overfitting, and it had data augmentation (rotations, translations, color variation) which increased robustness.

[AlexNet]: https://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf 
